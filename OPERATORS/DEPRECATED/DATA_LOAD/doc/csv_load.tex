\documentclass[letterpaper]{article}
\input{../../../latex/styles/ramesh_abbreviations}
\usepackage{times}
\usepackage{helvet}
\usepackage{courier}
\usepackage{hyperref}
\usepackage{fancyheadings}
\pagestyle{fancy}
\usepackage{pmc}
\usepackage{graphicx}
\setlength\textwidth{6.5in}
\setlength\textheight{8.5in}
\begin{document}
\title{Loading CSV files into Q}
\author{ Ramesh Subramonian }
\maketitle
\thispagestyle{fancy}
\lfoot{{\small Data Analytics Team}}
\cfoot{}
\rfoot{{\small \thepage}}


\begin{abstract}
A At a very high level, when we load data from a CSV file, we are
converting data presented in ascii in row format to a set of files in
binary format, one for each of the fields in the CSV
file. Technically, we are creating a table of vectors but we will get
to that detail later in this document.
\end{abstract}

\section{Introduction}

\subsection{Pre-Reading Material}

read \url{https://www.lua.org/pil/12.html}

\subsection{What is a CSV file?}
A CSV file is an ascii file such that:

\be
\item records are terminated by eoln character \verb+'\n'+
\item fields are separated by comma character
\item fields may be enclosed by a double-quote character 
e.g., \verb+1,"foo",2.3+
\item a single record can span multiple lines
\item a null value can be represented either by 2 consecutive 
double-quote characters e.g., the following lines are identical
\begin{verbatim}
foo,,2
foo,"",2
"foo,"","2"
\end{verbatim}
\item all rows must have the same number of fields 
\item Three characters have special meanings - comma, double quote and
  eoln. When we want these to occur as part of the value, they must be
  preceded by a back-slash. So, we really have 4 special
  characters. For example, the value \verb+ "abc\,\\\""+ has 6
  characters (1) a (2) b (3) c (4) comma (5) backslash and (6) double
  quote

\ee
\subsection{CSV Customizations}
\label{csv_custom}
At some future point, we might want to make the following customizable
\be
\item field separator, currently comma
\item record delimiter, currently eoln
\item field opener, currently double-quote
\item field closer, currently double-quote
\ee

\subsection{Environment Variables}
The following environment variables are mandatory
\be
\item \verb+Q_DATA_DIR+ This is where binary files corresponding to vectors will
  be stored. 
\item \verb+Q_META_DATA_DIR+ This is where meta data needed to restore state
  will be stored
\ee

\subsection{Conventions}
\label{conventions}

All binary files will 
\be
\item start with underscore 
\item be followed by a sequence of alphanumeric characters 
\item contain exactly 16 characters
\ee

\section{Q syntax}
The Q operator that loads a CSV file is as follows
\begin{verbatim}
X = load(data="foo.csv", meta_data = M, global_meta = M2 }
\end{verbatim}

We now explain M and M2. M2 has information that is pertinent to the processing
of the entire file. In addition to the information in Section~\ref{csv_custom},
it has 
\be
\item is\_hdr = true or false, which tells us whether to ignore the first line
  or not
\ee

M is a table that has \(N\) values, one for each of the \(N\) fields
that we expect to find in foo.csv

What does M look like?
\begin{verbatim}
M = { { name = "", qtype_type = "" },
{ name = "f1", null = "true", qtype = "I8" },
{ name = "f2", null = "false", qtype = "integer" },
{ name = "f3", qtype = "number" },
{ name = "f4", qtype = "SC", size = 16 },
{ name = "f5", qtype = "SV", dict = "D1", is_dict = false }, 
{ name = "f6", qtype = "SV", dict = "D2", is_dict = true, add=true},
{ name = "f7", qtype = "SV", dict = "D3", is_dict = true, add=false},
    }
\end{verbatim}

Let's go through these lines one at a time. The table has 6 fields.

\be
\item 
The first line says that the first field should be ignored - hence we
do not need to provide a name or a qtype
\item 
Line 2 for field f1 says that we expect all values of the second field to
be valid values for a variable of I4 and that the 
resulting vector is of qtype I4

\item 
Line 3 for field f2  says that we expect all values of the third field to
be valid values for a variable of qtype in 
\(\{\mathrm{I1, I2, I4, I8}\}\)
and that the
resulting vector is of type
\(\{\mathrm{I1, I2, I4, I8}\}\).
The smallest field that
can accomodate the values will be used. In other words, if all values are
between -32768 and +32767, then we will use I2
\framebox{Not for version 1. }
\item 
Line 4 for field f3 says that we expect all values of the third field to
be valid values for a variable of type F8 and that the resulting
vector is of type
\(\{\mathrm{I1, I2, I4, I8, F4, F8}\}\).
If all values
can be represented by an integer, the smallest integer field will be
used. Else, float will be used if it causes no loss; else, double.
\framebox{Not for version 1. }
\item Line 5 for field f4 says that this is a constant length string. The size
  includes the null character used to terminate the string. So, 0123456789ABCDE
  is a valid string for length 16 but 0123456789ABCDEDF is not. If the input
  value is less than 15 characters, it is right-padded with null characters so that
  the total length of the field (as written to the binary file) is 16
  characters. 

\item 
In order to explain lines with ``dict'', we need to introduce the concept of
a dictionary, Section~\ref{dictionary}
\ee

\subsection{Validations}

\begin{invariant}
If a given row of M does not have name specified or name is not null, then the
corresponding column will be ignored
\end{invariant}

\begin{invariant}
No validations on rows of meta data which are to be ignored
\end{invariant}

\begin{invariant}
  At least one column of the CSV file must not be ignored
\end{invariant}

\begin{invariant}
  {\bf name} must be unique
\end{invariant}

\begin{invariant}
  {\bf name} must contain only alphanumeric characters and underscore
\end{invariant}

\begin{invariant}
  {\bf qtype} must be specified and must be a valid value
\end{invariant}

\begin{invariant}
{\bf null} need not be specified. If not, assume false. If yes, then must
be either true or false, nothing else.
\end{invariant}

\begin{invariant}
If {\bf null} is true, then it is okay for data to have no null values.
If {\bf null} is false, then it is {\bf not} okay for data to have no null values.
either true or false, nothing else.
\end{invariant}

\begin{invariant}
If {\bf qtype} is SV, then {\bf is\_dict} must be specified as 
either true or or false. 
\end{invariant}



\section{Null values}
\label{null_values}

We now explain the {\bf null} in the meta data of the previous example. 

Let us say that a vector \(x\) has null values and is stored in file
\verb+_x+.  Then, we will have a nn file called \verb+_nn_x+ which
will tell us which values of \(x\) are null and which are not.

What does the nn file look like? Our initial idea was to use an 8-bit
integer to represent 1 and 0. We then decided to use bits
instead. However, there is an important subtlety regarding the size of
the nn file to be aware of.

If we had used a byte to record whether null or not, then if \(x\) had
1000 values and was of type \verb+int32_t+, then \verb+_x+ would be
4000 bytes in length and \verb+_nn_x+ would be 1000 bytes in
length. When the nn file is recorded as bits and not bytes, then we need to
{\bf round up} to the next multiple of 64. Since 1024 is the smallest
value greater than equal to 1000 which is a multiple of 64, then
\verb+_nn_x+ would have length 1024 bits = 128 bytes

What happens when the user specifies that a vector 
has no null values but we encounter a null value while reading the data
file? Abort the entire operation.

What happens when the user specifies that a vector 
has null values but we do {\bf not} encounter a null value while reading the
data? This is legitimate. However, we delete the nn file created before creating
the vector.

What happens if the user does not specify whether a vector has null values? In
this case, we assume that \verb+null = "true"+

For later: we will need these macros
\be 
\item \verb+set_bit(N, i, 1)+
\item \verb+set_bit(N, i, 0)+
\item \verb+get_bit(N, i)+
\ee

%-----------------------------------------------

\section{Dictionaries}
\label{dictionary}

Q does not (for now) support variable length character strings as a
data type.  So, when we read a field that contains a character string,
we convert it into an integer. A dictionary is the data structure that allows us
to deterministically map from strings to integers on the input and vice versa on
the output.

During loading, there are two main cases to consider
\be
\item \verb+is_dict=false+ no dictionary exists for this field and one must be created. If D1 is such
  a dictionary, then no other field in this load statement can use D1. If a
  dictionary called D1 exists, then this operation aborts.
\item \verb+is_dict=true+ a dictionary exists for this field and must be used. In this case, we have
  two sub-cases.  If you come across a string which does not exist in the dictionary, 
  \be
\item \verb+add=true+ can you add to the dictionary. 
\item \verb+add=false+ can you {\bf not} add to the dictionary. 
  \ee
  If D2 is such a dictionary, then it can be used by other field so long as they
  use it in a similar manner. If a dictonary D2 does not exist, the operation
  aborts. If \verb+add=false+ and you come across a string that is not in D2,
  the operation aborts.
  \ee

A few important implementation notes:
\be
\item Use Lua to parse the file up to the point where you have identified the
  string that corresponds to a particular cell in the table represented by the
  CSV file. At that time, hand off to C to convert this into a valid C type, as
  in Section~\ref{tbl_default_types}. As an example, the C function takes the
  string 
  \be
\item \verb+123456+ and creates the 4 bytes corresponding to an
  \verb+int32_t+ representation of 123456.
\item \verb+abc+ and creates the 8 bytes \verb+abc00000+ where 0 represents the
  null character and we assume that the user wanted a constant length string
  field of length 8.
  \ee


\item 
When the system shuts down, it is important that the dictionaries are persisted
to disk because they will need to be restored when the 
\item Our current approach relies on the dictionaries to be Lua tables i.e., we
  are relying on Lua's fairly efficient implementataion of hash tables. However,
  if the number and size of these fields becomes very large, then we will need
  to explore other alternatives e.g. other key value stores like Redis, LevelDB,
  \ldots
  \ee

\section{Custom Types}
\label{custom_types}

Types are registered globally with the following information. 
\be
\item short code 
\item C equivalent
\item description (optional)
\item input text converter. This is a C function which has the
  signature shown below. 
The function returns 0 if the conversion was successful and a
  negative number otherwise
  Assume that the short code
  is {\tt B} and that the C equivalent is \verb+int8_t+. In that case, the
  converter is 
  \begin{verbatim}
int txt_to_c( const char *in, int8_t *ptr_out)
\end{verbatim}
\item output text converter. This is a C function which has the
  signature shown below. 
The function returns 0 if the conversion was successful and a
  negative number otherwise
  Assume that the short code
  is {\tt B} and that the C equivalent is \verb+int8_t+. In that case, the
  converter is 
  \begin{verbatim}
  int c_to_txt(int8_t in, char *out, size_t sz_out)
\end{verbatim}
  \ee

The 6 types that come with the base Q installation are in
Table~\ref{tbl_default_types}

\begin{table}[hb]
\centering
\begin{tabular}{|l|l|l|l|l|l|} \hline \hline
  {\bf Short Code} & {\bf C Equivalent} & {\bf Description} %
  & {\bf Input Converter} & {\bf output Converter} \\ \hline \hline
  I1 & \verb+int8_t+ &  & strtod & fprintf, \%d\\ \hline
  I2 & \verb+int16_t+ & & strtod & fprintf, \%d\\ \hline
  I4 & \verb+int32_t+ & & strtod & fprintf, \%d\\ \hline
  I8 & \verb+int64_t+ & & strtoll & fprintf, \%lld \\ \hline
  F4 & \verb+float+ &   & strtof & fprintf, \%f\\ \hline
  F8 & \verb+double+ &  & strtold & fprintf, \%lf\\ \hline
  TM & struct tm & timestamp & strptime+ & strftime \\ \hline
\hline
\end{tabular}
\caption{Pre-built types}
\label{tbl_default_types}
\end{table}





\end{document}
