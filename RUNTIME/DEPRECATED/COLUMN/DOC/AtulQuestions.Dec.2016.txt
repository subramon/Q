1. Compilation failed on ubuntu 14.04 .. easilyget_val fixed by
commenting out offending line 

>> I will send you the latest code that compiles cleanly. It has
fallen into a bit of disuse and is not as clean as when I was working
seriously on it.

2. "get_val" not implemented?

>> It is implemented in the code I will send you

3. what does NN field value stand for?
   - undefined?
   - why does it refer to other field?

NN stands for not-null. Having null data is a fact of life in data
analytics.  Hence, for every field foo, there is a field called
_nn_foo. While this could be a bit vector, for now it is a byte
vector. So, _nn_foo[i] = 1 means that foo[i] is not null; if it is 0,
then it means that it is null. No other values are allowed in
_nn_foo[i].

If a field has no null values, then we do not store the nn field. 

4. given the kind of vector manipulation operations Q supports, would
    it make sense to add following frequent (?) operations

(X, fc) -> Y with all indices where fc=true

>>> Why won't copy_fld handle this case? I tried to implement both
    versions and let the Q programmer pick. Take the call f1f2_to_s as
    an example. What it does is to sup up f1 but only when f2 is
    non-null (f2 is expected to be a boolean field). However, one
    could have done "q copy_fld t1 f1 f2 t2" and then "q f_to_s t2 f1 sum"

     * I noticed many operations have a condition field,
       where you go over the entire vector and operations
       are executed only when condition is true

     * for repeated iterations over the condition, it might
       make sense to compute true-indices once (in Y) and
        use the much smaller Y in subsequent ops .. especially
       useful where the 'true' values are sparse

  - this can be complemented by a "lookup" function s.t
       Z(i) = X(Y(i)) ... similar to "xfer" function perhaps?

5. You mentioned perf optimization is an important goal of Q. A few
    things came to my mind from a quick glance over the C-code (based
    on my limited knowledge...)

  - would it make sense to optimize the condition checks
     in "copy_fld" and other places to check upto 4-8 byte
     values instead of iterating over each byte for each value?
    * use single I8 comparison (comparing 8 bytes at a time,
      supported natively by the instruction set)

         ex. char * condptr; // ensure alignment
               long *cond = (long *) condptr;
               switch (cond & 0x0101010101010101) {
                   .....
               }

>>> This I am not convinced about. You will have to give me more than
    a code snippet to persuade me :-)


  - "exact regex_match" could be similarly optimized by computing
     strlen() of both src and dest first .... and short-circuiting
     strcmp() if lengths mismatch; also eliminates null-seeking
     pointer increment loop since lengths are known in advance.

>>> This I 100% agree with. This was built to handle lookup tables
    where the user gives a string and we want to convert it into an
    integer. There are definitely much better ways of doing it. The
    reason I didn't optimize it was that these lookup tables were
    relatively small and this call happened only in the beginning of a
    sequence of complex operations and hence was a small overhead.

>>> But this does raise an imporant part of the Q design
    philosophy. The aim is to get something *correct* working quickly
    but to do it in a way that performance optimization could come in
    later without wildly affecting system stability.

6. what is special about "mk_bins" .. does it perform optimal binning
like histograms?


7. I see some referenced to CILK in the code ... but no use of actual
    keyworkds (ex. cilk_for). Have you tried that out yet?

I did try that but I dropped it in favor of openmp which was easier to
distribute (at that time cilkplus was still proprietary). Current thinking, based on cilkplus.org is

Next, look at what you want to make parallel. Use OpenMP if the
parallelism is primarily for bounded loops over built-in types, or if
it is flat do-loop centric parallelism. OpenMP works especially well
with large and predictable data parallel problems. It can be very
challenging to match OpenMP performance with Cilk Plus or TBB for such
problems. It is seldom worth the effort to bother â€“ just use OpenMP.
Cilk Plus and TBB excels at less structured or consistent parallelism.

Sorry if some of the questions seem crazy/obvious ... just that a
first look at the code/impl triggered these thoughts.

Do send us the "logistic regression" paper you referred to during our
call.

Thanks,
Atul
