\section{Evaluating a decision tree}
In addition to the standard metrics, such as classification accuracy, we
evaluate decision trees as follows. Every element of the data set is 
fed into the decision tree, thereby
assigning each one to a leaf. Now, for each leaf, we have four numbers.
\be
\item \(n^H_1, n^T_1\) which is the number of testing samples assigned 
to the leaf that were labeled heads and tails respectively. The ``weight'' of a
leaf is \(n^H_1 + n^T_1\)
\item \(n^H_0, n^T_0\) same as above but for the training samples
\ee

These numbers are used follows. Continuing our gambling metaphor,
the house is given only \(n^H_0, n^T_0\) to set the odds, whereas the gambler
has access to all four.
The ``true'' probability of heads, \(\rho\), is assumed to be 
\(\frac{n^H_1}{n^H_1+ n^T_1}\). 

Consider three examples, in all of which \(n^H_0, n^T_0 = 60, 40\). 
The benefit, \(b\) that the gambler can expect to make per trial varies
depending on \(rho\) as follows.
\be
\item \(\rho = 0.8\). The adversary would bet heads.
\(b = 0.8 \times \frac{40}{60} + 0.2 \times -1 = \frac{1}{3}\)
\item \(\rho = 0.6\). If the adversary bet heads, 
\(b = 0.6 \times \frac{40}{60} + 0.4 \times -1 = 0\)
If the adversary bet tails, 
\(b = 0.6 \times -1 + 0.4 \times \frac{60}{40} = 0\).
\item \(\rho = 0.4\). The adversary would bet tails.
\(b = 0.4 \times -1 + 0.6 \times \frac{60}{40} = \frac{1}{2}\) 
\ee

The weighted benefit of a leaf is simply \(w \times b\).
The ``cost'' of the decision tree is sum over the 
weighted benefit, normalized by the weights of all leaves 
\(= \frac{\sum_l w_l \times b_l}{\sum_l w_l}\)

